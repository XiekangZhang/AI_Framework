{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distributed_training.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FohCev-137Wv",
        "colab_type": "text"
      },
      "source": [
        "#1. Distributed training with Keras\n",
        "\n",
        "API provides an abstraction for distributing your training across multiple processing units. The goal is to allow users to enable distributed training using existing models and training code, with minimal changes. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPExf5--3jnS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "f322db3f-c33d-4912-997f-71a31d2a4502"
      },
      "source": [
        "!pip install -q tf-nightly\n",
        "\n",
        "%tensorflow_version 2.x\n",
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "tfds.disable_progress_bar()\n",
        "\n",
        "import os"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 519.0MB 31kB/s \n",
            "\u001b[K     |████████████████████████████████| 460kB 39.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.0MB 39.7MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oPjiFiJ5eDv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e7a49cf9-cd37-4efd-c7fb-8d3161eab652"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.2.0-dev20200419\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2wxTkDA5zFk",
        "colab_type": "text"
      },
      "source": [
        "##1.1. Download the dataset\n",
        "\n",
        "Setting with_info to True includes the medadata for the entiredataset, which is being saved here to info. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f5UFKr045h4p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        },
        "outputId": "5088c699-d675-4b9b-8957-d58934e51983"
      },
      "source": [
        "datasets, info = tfds.load(name='mnist', with_info=True, as_supervised=True)\n",
        "mnist_train, mnist_test = datasets['train'], datasets['test']"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:absl:Dataset mnist is hosted on GCS. It will automatically be downloaded to your\n",
            "local data directory. If you'd instead prefer to read directly from our public\n",
            "GCS bucket (recommended if you're running on GCP), you can instead set\n",
            "data_dir=gs://tfds-data/datasets.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[1mDownloading and preparing dataset mnist/3.0.0 (download: 11.06 MiB, generated: Unknown size, total: 11.06 MiB) to /root/tensorflow_datasets/mnist/3.0.0...\u001b[0m\n",
            "\u001b[1mDataset mnist downloaded and prepared to /root/tensorflow_datasets/mnist/3.0.0. Subsequent calls will reuse this data.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "um_k-u_W6UKT",
        "colab_type": "text"
      },
      "source": [
        "##1.2. Define distribution strategy\n",
        "\n",
        "Create a MirroredStrategy object. This will handle distribution, and provides a context manager to build your model inside."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RgNHIGYH6RNf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "997e9356-9b6f-4bfe-83c7-3cda7663de0c"
      },
      "source": [
        "strategy = tf.distribute.MirroredStrategy()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:There are non-GPU devices in `tf.distribute.Strategy`, not using nccl allreduce.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:CPU:0',)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8lpI7Nr6t7b",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "2694b553-1ffa-4c1e-d1de-0cc3af675163"
      },
      "source": [
        "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of devices: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWV38yMI624y",
        "colab_type": "text"
      },
      "source": [
        "##1.3. Setup input pipeline\n",
        "\n",
        "When training a model with multiple GPUs, you can use the extra computing power effectively by increasing the batch size. In general, use the largest batch size that fits the GPU memory, and tune the learning rate accordingly. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kure8Rbi60A4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# You can also do info.splits.total_num_examples to get the total\n",
        "# number of examples in the dataset.\n",
        "\n",
        "num_train_examples = info.splits['train'].num_examples\n",
        "num_test_examples = info.splits['test'].num_examples\n",
        "\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "BATCH_SIZE_PER_REPLICA = 64\n",
        "BATCH_SIZE = BATCH_SIZE_PER_REPLICA * strategy.num_replicas_in_sync"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C8wq6gvB75OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Pixel values, which are 0-255, have to be normalized to the 0-1 range\n",
        "def scale(image, label):\n",
        "  image = tf.cast(image, tf.float32)\n",
        "  image /= 255\n",
        "  \n",
        "  return image, label"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "esugZLUr85T4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply this function to the training and test data. \n",
        "# shuffle the training data, and batch it for training. \n",
        "# Notice we are alos keeping an in-memory cache of the training data to improve performance.\n",
        "\n",
        "train_dataset = mnist_train.map(scale).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
        "eval_dataset = mnist_test.map(scale).batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgWmRw1m9x03",
        "colab_type": "text"
      },
      "source": [
        "##1.4. Create the model\n",
        "\n",
        "Create and compile the Keras model in the context of strategy.scope"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS4jIY2n9wby",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "with strategy.scope():\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Conv2D(32, 3, activation='relu', input_shape=(28, 28, 1)),\n",
        "      tf.keras.layers.MaxPooling2D(),\n",
        "      tf.keras.layers.Flatten(),\n",
        "      tf.keras.layers.Dense(64, activation='relu'),\n",
        "      tf.keras.layers.Dense(10)\n",
        "  ])\n",
        "\n",
        "  model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TThViyee-B-d",
        "colab_type": "text"
      },
      "source": [
        "##1.5. Define the callbacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kvei3pSJ-Aat",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define the checkpoint directory to store the checkpoints\n",
        "\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5x8_Py-H-L87",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Function for decaying the learning rate.\n",
        "# You can define any decay function you need.\n",
        "def decay(epoch):\n",
        "  if epoch < 3:\n",
        "    return 1e-3\n",
        "  elif epoch >= 3 and epoch < 7:\n",
        "    return 1e-4\n",
        "  else:\n",
        "    return 1e-5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jmou36KD-S0n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Callback for printing the LR at the end of each epoch.\n",
        "class PrintLR(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs=None):\n",
        "    print('\\nLearning rate for epoch {} is {}'.format(epoch + 1,\n",
        "                                                      model.optimizer.lr.numpy()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "co3Uwq_x-kX2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "callbacks = [\n",
        "    tf.keras.callbacks.TensorBoard(log_dir='./logs'),\n",
        "    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,\n",
        "                                       save_weights_only=True),\n",
        "    tf.keras.callbacks.LearningRateScheduler(decay),\n",
        "    PrintLR()\n",
        "]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kMuHqjcT-qnT",
        "colab_type": "text"
      },
      "source": [
        "##1.6. Train and evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhdHNSZa-oe0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "cf49670a-4129-4246-d78a-48b9aa18cb8d"
      },
      "source": [
        "model.fit(train_dataset, epochs=12, callbacks=callbacks)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "  1/938 [..............................] - ETA: 0s - loss: 2.2961 - accuracy: 0.1875WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1271: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py:1271: stop (from tensorflow.python.eager.profiler) is deprecated and will be removed after 2020-07-01.\n",
            "Instructions for updating:\n",
            "use `tf.profiler.experimental.stop` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "937/938 [============================>.] - ETA: 0s - loss: 0.2095 - accuracy: 0.9388\n",
            "Learning rate for epoch 1 is 0.0010000000474974513\n",
            "938/938 [==============================] - 31s 33ms/step - loss: 0.2093 - accuracy: 0.9388\n",
            "Epoch 2/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0658 - accuracy: 0.9806\n",
            "Learning rate for epoch 2 is 0.0010000000474974513\n",
            "938/938 [==============================] - 25s 27ms/step - loss: 0.0658 - accuracy: 0.9806\n",
            "Epoch 3/12\n",
            "936/938 [============================>.] - ETA: 0s - loss: 0.0460 - accuracy: 0.9862\n",
            "Learning rate for epoch 3 is 0.0010000000474974513\n",
            "938/938 [==============================] - 25s 27ms/step - loss: 0.0460 - accuracy: 0.9862\n",
            "Epoch 4/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0251 - accuracy: 0.9929\n",
            "Learning rate for epoch 4 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 25s 27ms/step - loss: 0.0251 - accuracy: 0.9929\n",
            "Epoch 5/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0219 - accuracy: 0.9940\n",
            "Learning rate for epoch 5 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 25s 27ms/step - loss: 0.0219 - accuracy: 0.9940\n",
            "Epoch 6/12\n",
            "938/938 [==============================] - ETA: 0s - loss: 0.0200 - accuracy: 0.9947\n",
            "Learning rate for epoch 6 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 25s 27ms/step - loss: 0.0200 - accuracy: 0.9947\n",
            "Epoch 7/12\n",
            "936/938 [============================>.] - ETA: 0s - loss: 0.0185 - accuracy: 0.9953\n",
            "Learning rate for epoch 7 is 9.999999747378752e-05\n",
            "938/938 [==============================] - 27s 29ms/step - loss: 0.0185 - accuracy: 0.9953\n",
            "Epoch 8/12\n",
            "936/938 [============================>.] - ETA: 0s - loss: 0.0160 - accuracy: 0.9963\n",
            "Learning rate for epoch 8 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 26s 27ms/step - loss: 0.0160 - accuracy: 0.9963\n",
            "Epoch 9/12\n",
            "936/938 [============================>.] - ETA: 0s - loss: 0.0158 - accuracy: 0.9964\n",
            "Learning rate for epoch 9 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 25s 27ms/step - loss: 0.0157 - accuracy: 0.9964\n",
            "Epoch 10/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0156 - accuracy: 0.9964\n",
            "Learning rate for epoch 10 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 26s 27ms/step - loss: 0.0155 - accuracy: 0.9964\n",
            "Epoch 11/12\n",
            "936/938 [============================>.] - ETA: 0s - loss: 0.0154 - accuracy: 0.9965\n",
            "Learning rate for epoch 11 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 25s 27ms/step - loss: 0.0154 - accuracy: 0.9965\n",
            "Epoch 12/12\n",
            "937/938 [============================>.] - ETA: 0s - loss: 0.0152 - accuracy: 0.9964\n",
            "Learning rate for epoch 12 is 9.999999747378752e-06\n",
            "938/938 [==============================] - 25s 27ms/step - loss: 0.0152 - accuracy: 0.9964\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fcfaa33b9b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XayGnOhA-wgV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "495a7b8c-bc3f-4999-bcc3-7647b62533dd"
      },
      "source": [
        "# check the checkpoint directory\n",
        "!ls {checkpoint_dir}"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint\t\t     ckpt_4.data-00000-of-00001\n",
            "ckpt_10.data-00000-of-00001  ckpt_4.index\n",
            "ckpt_10.index\t\t     ckpt_5.data-00000-of-00001\n",
            "ckpt_11.data-00000-of-00001  ckpt_5.index\n",
            "ckpt_11.index\t\t     ckpt_6.data-00000-of-00001\n",
            "ckpt_12.data-00000-of-00001  ckpt_6.index\n",
            "ckpt_12.index\t\t     ckpt_7.data-00000-of-00001\n",
            "ckpt_1.data-00000-of-00001   ckpt_7.index\n",
            "ckpt_1.index\t\t     ckpt_8.data-00000-of-00001\n",
            "ckpt_2.data-00000-of-00001   ckpt_8.index\n",
            "ckpt_2.index\t\t     ckpt_9.data-00000-of-00001\n",
            "ckpt_3.data-00000-of-00001   ckpt_9.index\n",
            "ckpt_3.index\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9649cb___J34",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "39fba44d-63d3-4d6f-b646-4150a1528cde"
      },
      "source": [
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "eval_loss, eval_acc = model.evaluate(eval_dataset)\n",
        "\n",
        "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 3s 22ms/step - loss: 0.0358 - accuracy: 0.9882\n",
            "Eval loss: 0.03579382598400116, Eval Accuracy: 0.9882000088691711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPilpRDM_QWi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        },
        "outputId": "cf3fdf39-2a4b-4bfd-e01e-7f2aea030c28"
      },
      "source": [
        "!tensorboard --logdir=/content/logs"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/tensorboard\", line 8, in <module>\n",
            "    sys.exit(run_main())\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/main.py\", line 75, in run_main\n",
            "    app.run(tensorboard.main, flags_parser=tensorboard.configure)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 299, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/absl/app.py\", line 250, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 289, in main\n",
            "    return runner(self.flags) or 0\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 305, in _run_serve_subcommand\n",
            "    server = self._make_server()\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/program.py\", line 409, in _make_server\n",
            "    self.flags, self.plugin_loaders, self.assets_zip_provider\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 183, in standard_tensorboard_wsgi\n",
            "    flags, plugin_loaders, data_provider, assets_zip_provider, multiplexer\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 272, in TensorBoardWSGIApp\n",
            "    tbplugins, flags.path_prefix, data_provider, experimental_plugins\n",
            "  File \"/usr/local/lib/python3.6/dist-packages/tensorboard/backend/application.py\", line 345, in __init__\n",
            "    \"Duplicate plugins for name %s\" % plugin.plugin_name\n",
            "ValueError: Duplicate plugins for name projector\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x604FjTWHT2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "3f151fd2-80b9-44b8-b820-3e97bf516537"
      },
      "source": [
        "ls -sh ./logs"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4.0K\n",
            "4.0K \u001b[0m\u001b[01;34mtrain\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh6EBRQZWnv5",
        "colab_type": "text"
      },
      "source": [
        "##1.7. Export to SavedModel"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-d9dCFhoWlkd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "59655489-9c58-46f6-e105-05ac1da9a547"
      },
      "source": [
        "path = 'saved_model/'\n",
        "model.save(path, save_format='tf')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: saved_model/assets\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGRoHXcxWzBo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "99c5588b-5ae1-439b-c4bd-3d8e7706bfbf"
      },
      "source": [
        "# Load model without strategy.scope\n",
        "unreplicated_model = tf.keras.models.load_model(path)\n",
        "\n",
        "unreplicated_model.compile(\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    optimizer=tf.keras.optimizers.Adam(),\n",
        "    metrics=['accuracy'])\n",
        "\n",
        "eval_loss, eval_acc = unreplicated_model.evaluate(eval_dataset)\n",
        "\n",
        "print('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 3s 22ms/step - loss: 0.0358 - accuracy: 0.9882\n",
            "Eval loss: 0.03579382598400116, Eval Accuracy: 0.9882000088691711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sc43XHqpW8wh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7dc725c7-9773-423a-a9bb-6729a5b29506"
      },
      "source": [
        "# Load with strategy.scope\n",
        "with strategy.scope():\n",
        "  replicated_model = tf.keras.models.load_model(path)\n",
        "  replicated_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "                           optimizer=tf.keras.optimizers.Adam(),\n",
        "                           metrics=['accuracy'])\n",
        "\n",
        "  eval_loss, eval_acc = replicated_model.evaluate(eval_dataset)\n",
        "  print ('Eval loss: {}, Eval Accuracy: {}'.format(eval_loss, eval_acc))"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "157/157 [==============================] - 3s 22ms/step - loss: 0.0358 - accuracy: 0.9882\n",
            "Eval loss: 0.03579382598400116, Eval Accuracy: 0.9882000088691711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-1PdMK4XCRA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}